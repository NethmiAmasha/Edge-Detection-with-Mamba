{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrc9xvg6P2mCVWCarWXOsC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NethmiAmasha/Edge-Detection-with-Mamba/blob/main/EDMB_fromscratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hm5fQzH9cxb",
        "outputId": "0ab7c864-dd1d-47c2-a72e-fb9fbf57ef07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda available: True\n",
            "torch: 2.8.0+cu126\n",
            "cuda device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "# Colab cell\n",
        "import torch\n",
        "print(\"cuda available:\", torch.cuda.is_available())\n",
        "print(\"torch:\", torch.__version__)\n",
        "print(\"cuda device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q torch torchvision"
      ],
      "metadata": {
        "id": "y5_jM10ZJCbY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q tqdm opencv-python matplotlib pillow scipy"
      ],
      "metadata": {
        "id": "om87ZKRPJMEV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q git+https://github.com/Dao-AILab/causal-conv1d@v1.1.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Bd9OKviJRZ-",
        "outputId": "2411cbde-7e93-4666-f3d3-f60ab7a6d74b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.5/446.5 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for causal_conv1d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for buildtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "High-resolution encoder (Eh)"
      ],
      "metadata": {
        "id": "EF3jaeqtQVhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class HighResEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Minimal high-resolution encoder (Eh).\n",
        "    Produces two feature maps:\n",
        "      - f1: same spatial resolution as input (useful for edges)\n",
        "      - f2: half resolution (downsampled)\n",
        "    This is intentionally simple so you can read & understand every line.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch=3, base_ch=16):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, base_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(base_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(base_ch, base_ch*2, kernel_size=3, stride=2, padding=1),  # downsamples by 2\n",
        "            nn.BatchNorm2d(base_ch*2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # small extra conv to produce a refined same-res feature from upsampled low-res\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.Conv2d(base_ch*2, base_ch, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(base_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B,3,H,W)\n",
        "        f1 = self.conv1(x)               # (B, base_ch, H, W)\n",
        "        f2 = self.conv2(f1)              # (B, base_ch*2, H/2, W/2)\n",
        "        # upsample f2 back to input resolution and refine\n",
        "        f2_up = F.interpolate(f2, size=f1.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        f2_refined = self.refine(f2_up)  # (B, base_ch, H, W)\n",
        "        # return a list of features consistent with paper notation [f1 (HR), f2 (down->HR)]\n",
        "        return [f1, f2_refined]\n",
        "\n",
        "# Quick shape test\n",
        "if __name__ == \"__main__\":\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = HighResEncoder(in_ch=3, base_ch=16).to(device)\n",
        "    x = torch.randn(2,3,320,320).to(device)   # batch of 2, 320x320 images\n",
        "    feats = model(x)\n",
        "    print(\"f1 shape:\", feats[0].shape)\n",
        "    print(\"f2_refined shape:\", feats[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWDslIxDQOcU",
        "outputId": "1ac25fc7-c31a-47c3-fecb-d71544bc155d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 shape: torch.Size([2, 16, 320, 320])\n",
            "f2_refined shape: torch.Size([2, 16, 320, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Global Mamba Encoder (Eg) and Fine-grained Mamba Encoder (Ef)"
      ],
      "metadata": {
        "id": "LXHTKCe5SlOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplified MIXENC (Global + Fine-grained encoders)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleCNNEncoder(nn.Module):\n",
        "    \"\"\"A lightweight CNN that returns 3 feature maps at different scales.\"\"\"\n",
        "    def __init__(self, in_ch=3, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.stage1 = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, base_ch, 3, 1, 1),\n",
        "            nn.BatchNorm2d(base_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.stage2 = nn.Sequential(\n",
        "            nn.Conv2d(base_ch, base_ch*2, 3, 2, 1),  # downsample by 2\n",
        "            nn.BatchNorm2d(base_ch*2),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.stage3 = nn.Sequential(\n",
        "            nn.Conv2d(base_ch*2, base_ch*4, 3, 2, 1),  # downsample by 4\n",
        "            nn.BatchNorm2d(base_ch*4),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        f1 = self.stage1(x)  # (B, base_ch, H, W)\n",
        "        f2 = self.stage2(f1) # (B, base_ch*2, H/2, W/2)\n",
        "        f3 = self.stage3(f2) # (B, base_ch*4, H/4, W/4)\n",
        "        return [f1, f2, f3]\n",
        "\n",
        "class MIXENC_Light(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified version of MIXENC (global + local encoders) for understanding.\n",
        "    Mimics structure of Eg (global) + Ef (fine-grained).\n",
        "    \"\"\"\n",
        "    def __init__(self, base_ch=32):\n",
        "        super().__init__()\n",
        "        self.global_encoder = SimpleCNNEncoder(in_ch=3, base_ch=base_ch)\n",
        "        self.local_encoder = SimpleCNNEncoder(in_ch=3, base_ch=base_ch)\n",
        "\n",
        "    def cat_patch(self, f00, f01, f10, f11, target_size):\n",
        "        \"\"\"Stitch 4 patches back together and resize to match global feature size.\"\"\"\n",
        "        top = torch.cat([f00, f01], dim=3)\n",
        "        bottom = torch.cat([f10, f11], dim=3)\n",
        "        combined = torch.cat([top, bottom], dim=2)\n",
        "        combined = F.interpolate(combined, size=target_size, mode='bilinear', align_corners=False)\n",
        "        return combined\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Returns:\n",
        "          global_feats: list of feature maps from global encoder\n",
        "          local_feats: list of patch-fused feature maps\n",
        "        \"\"\"\n",
        "        # 1️⃣ Global features\n",
        "        global_feats = self.global_encoder(x)\n",
        "\n",
        "        # 2️⃣ Prepare local patches\n",
        "        _, _, H, W = x.shape\n",
        "        x_up = F.interpolate(x, scale_factor=1.2, mode='bilinear', align_corners=False)\n",
        "        _, _, H2, W2 = x_up.shape\n",
        "        h_mid, w_mid = H2 // 2, W2 // 2\n",
        "\n",
        "        patches = [\n",
        "            x_up[..., :h_mid, :w_mid],   # top-left\n",
        "            x_up[..., :h_mid, w_mid:],   # top-right\n",
        "            x_up[..., h_mid:, :w_mid],   # bottom-left\n",
        "            x_up[..., h_mid:, w_mid:]    # bottom-right\n",
        "        ]\n",
        "\n",
        "        # 3️⃣ Extract local features for each patch\n",
        "        local_feats_per_patch = [self.local_encoder(p) for p in patches]  # list of 4 lists\n",
        "\n",
        "        # 4️⃣ Merge patch features layer-wise\n",
        "        local_feats = []\n",
        "        for i in range(len(global_feats)):\n",
        "            f00, f01, f10, f11 = local_feats_per_patch[0][i], local_feats_per_patch[1][i], local_feats_per_patch[2][i], local_feats_per_patch[3][i]\n",
        "            merged = self.cat_patch(f00, f01, f10, f11, target_size=global_feats[i].shape[2:])\n",
        "            local_feats.append(merged)\n",
        "\n",
        "        return global_feats, local_feats\n",
        "\n",
        "# 🔍 Test with random input\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = MIXENC_Light(base_ch=32).to(device)\n",
        "x = torch.randn(1, 3, 320, 320).to(device)\n",
        "\n",
        "global_feats, local_feats = model(x)\n",
        "for i, (g, l) in enumerate(zip(global_feats, local_feats)):\n",
        "    print(f\"Level {i+1}: global {g.shape}, local {l.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmMrCF-zVTDs",
        "outputId": "ce1de51f-121b-4979-ab49-298a925457df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Level 1: global torch.Size([1, 32, 320, 320]), local torch.Size([1, 32, 320, 320])\n",
            "Level 2: global torch.Size([1, 64, 160, 160]), local torch.Size([1, 64, 160, 160])\n",
            "Level 3: global torch.Size([1, 128, 80, 80]), local torch.Size([1, 128, 80, 80])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# High-resolution encoder (Eh)\n",
        "model_Eh = HighResEncoder(in_ch=3, base_ch=16).to(device)\n",
        "\n",
        "# Combined Global + Fine-grained encoder (Eg + Ef)\n",
        "model_MIX = MIXENC_Light(base_ch=32).to(device)"
      ],
      "metadata": {
        "id": "tmFBEy9kX8rm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test both encoders and print feature shapes\n",
        "with torch.no_grad():\n",
        "    x = torch.randn(1, 3, 320, 320).to(device)\n",
        "    f_high = model_Eh(x)\n",
        "    g_global, g_local = model_MIX(x)\n",
        "\n",
        "print(\"Eh outputs:\")\n",
        "for i, f in enumerate(f_high):\n",
        "    print(f\"  f{i+1}: {list(f.shape)}\")\n",
        "\n",
        "print(\"\\nEg/Ef outputs (local_feats):\")\n",
        "for i, f in enumerate(g_local):\n",
        "    print(f\"  level{i+1}: {list(f.shape)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKWkf_xnYBoP",
        "outputId": "4e88f4eb-cba0-4219-9cad-88215e13fc95"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eh outputs:\n",
            "  f1: [1, 16, 320, 320]\n",
            "  f2: [1, 16, 320, 320]\n",
            "\n",
            "Eg/Ef outputs (local_feats):\n",
            "  level1: [1, 32, 320, 320]\n",
            "  level2: [1, 64, 160, 160]\n",
            "  level3: [1, 128, 80, 80]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learnable Gaussian Distribution Decoder"
      ],
      "metadata": {
        "id": "QhBvbxUoZjkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LGDDecoder_Light(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Learnable Gaussian Distribution (LGD) decoder.\n",
        "    Fuses high-res (Eh) and global (Eg/Ef) features.\n",
        "    Outputs mu, sigma², and an auxiliary edge map.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch_high=16, in_ch_global=128, mid_ch=64):\n",
        "        super().__init__()\n",
        "        # 1️⃣ Fuse features from Eh and Eg/Ef\n",
        "        self.fuse = nn.Sequential(\n",
        "            nn.Conv2d(in_ch_high + in_ch_global, mid_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(mid_ch),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_ch, mid_ch, 3, padding=1),\n",
        "            nn.BatchNorm2d(mid_ch),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # 2️⃣ Output heads\n",
        "        self.mu_head = nn.Conv2d(mid_ch, 1, kernel_size=1)      # mean\n",
        "        self.var_head = nn.Sequential(                          # variance (>=0)\n",
        "            nn.Conv2d(mid_ch, 1, kernel_size=1),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.edge_head = nn.Conv2d(mid_ch, 1, kernel_size=1)    # auxiliary edge map\n",
        "\n",
        "    def forward(self, f_high, f_global):\n",
        "        # Upsample global feature to match high-res size\n",
        "        f_global_up = F.interpolate(f_global, size=f_high.shape[-2:], mode='bilinear', align_corners=False)\n",
        "        fused = torch.cat([f_high, f_global_up], dim=1)\n",
        "        x = self.fuse(fused)\n",
        "        mu = self.mu_head(x)\n",
        "        sigma2 = self.var_head(x)\n",
        "        edge_logits = self.edge_head(x)\n",
        "        return mu, sigma2, edge_logits\n",
        "\n",
        "# 🔍 Test with random features\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "decoder = LGDDecoder_Light(in_ch_high=16, in_ch_global=128, mid_ch=64).to(device)\n",
        "\n",
        "# fake inputs from your encoders\n",
        "f_high = torch.randn(1, 16, 320, 320).to(device)\n",
        "f_global = torch.randn(1, 128, 80, 80).to(device)\n",
        "\n",
        "mu, sigma2, edge_logits = decoder(f_high, f_global)\n",
        "print(\"mu:\", mu.shape, \"sigma2:\", sigma2.shape, \"edge_logits:\", edge_logits.shape)\n",
        "print(\"sigma2 min/max:\", sigma2.min().item(), sigma2.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLZPLjmBZiwk",
        "outputId": "43939271-fbcf-4b47-ae02-77c134a57b0b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mu: torch.Size([1, 1, 320, 320]) sigma2: torch.Size([1, 1, 320, 320]) edge_logits: torch.Size([1, 1, 320, 320])\n",
            "sigma2 min/max: 0.24327895045280457 1.2697547674179077\n"
          ]
        }
      ]
    }
  ]
}